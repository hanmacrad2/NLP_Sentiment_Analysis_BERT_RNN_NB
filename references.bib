@article{cambria2017sentiment,
  title={Sentiment analysis is a big suitcase},
  author={Cambria, Erik and Poria, Soujanya and Gelbukh, Alexander and Thelwall, Mike},
  journal={IEEE Intelligent Systems},
  volume={32},
  number={6},
  pages={74--80},
  year={2017},
  publisher={IEEE}
}

@article{carvalho2020evaluation,
  title={On the evaluation and combination of state-of-the-art features in Twitter sentiment analysis},
  author={Carvalho, Jonnathan and Plastino, Alexandre},
  journal={Artificial Intelligence Review},
  pages={1--50},
  year={2020},
  publisher={Springer}
}

@online{EugenioCulurciello,
    author = "Eugenio Culurciello",
    title = "The fall of RNN / LSTM",
    url  = "https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0",
    addendum = "(accessed: 08.04.2021)",
    keywords = "latex,"
}


@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{gers1999learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  year={1999},
  publisher={IET}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{li2020incorporating,
  title={Incorporating stock prices and news sentiments for stock market prediction: A case of Hong Kong},
  author={Li, Xiaodong and Wu, Pangjing and Wang, Wenpeng},
  journal={Information Processing \& Management},
  volume={57},
  number={5},
  pages={102212},
  year={2020},
  publisher={Elsevier}
}

@article{lochter2016short,
  title={Short text opinion detection using ensemble of classifiers and semantic indexing},
  author={Lochter, Johannes V and Zanetti, Rafael F and Reller, Dominik and Almeida, Tiago A},
  journal={Expert Systems with Applications},
  volume={62},
  pages={243--249},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{mbn_vs_bnb,
  author = {McCallum, Andrew and Nigam, Kamal},
  booktitle = {Learning for Text Categorization: Papers from the 1998 {AAAI} Workshop },
  keywords = {bayes bernoulli classification ereignis event model multinomial Naive text vergleich},
  pages = {41--48},
  timestamp = {2013-11-23T20:11:51.000+0100},
  title = {A Comparison of Event Models for Na\"ive {B}ayes Text Classification},
  url = {http://www.kamalnigam.com/papers/multinomial-aaaiws98.pdf},
  year = 1998
}

@online{NikBert,
    author = "Nikolai Yakovenko",
    title = "The Deep Natural Language Papers You Need to Read — BERT, GPT-2 and looking forward",
    url  = "https://moscow25.medium.com/the-best-deep-natural-language-papers-you-should-read-bert-gpt-2-and-looking-forward-1647f4438797#:~:text=Google's",
    addendum = "(accessed: 08.04.2021)",
    keywords = "latex,"
}

@article{pang2002thumbs,
  title={Thumbs up? Sentiment classification using machine learning techniques},
  author={Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
  journal={arXiv preprint cs/0205070},
  year={2002}
}

@inproceedings{tumasjan2010predicting,
  title={Predicting elections with twitter: What 140 characters reveal about political sentiment},
  author={Tumasjan, Andranik and Sprenger, Timm and Sandner, Philipp and Welpe, Isabell},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={4},
  number={1},
  year={2010}
}

@article{turney2002thumbs,
  title={Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews},
  author={Turney, Peter D},
  journal={arXiv preprint cs/0212032},
  year={2002}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@article{yin2017comparative,
  title={Comparative study of CNN and RNN for natural language processing},
  author={Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1702.01923},
  year={2017}
}

@article{Hochreiter1997,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}